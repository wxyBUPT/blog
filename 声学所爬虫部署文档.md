#声学所爬虫部署文档


* [基础环境准备](#4) 
 * [更换源](#4.2)
 * [python](#4.1)  
 * [升级python版本](#4.1.1)  
 * [python环境与python依赖](#4.1.2)
 * [nodeJs](#4.2)
 * [mongo ](#4.3)
 * [supervisor](#4.4)
 * [配置crontab](#4.5)
 * [防火墙配置与相关的端口配置](#4.6)


* [点播爬虫](#1)  

 * [概述](#1.1)  
 * [部署](#1.2)
 * [配置](#1.3)
 * [调试运行](#1.4)
 * [配置supervisor与crontab](#1.5)

* [点播媒体文件下载](#2)
 * [](#2.1)
 * [](#2.2)

* [页面统计后台](#3)  

 * [统计后台概述](#3.1)
 * [统计后台配置](#3.2)

* [页面统计前端接口](#5)  

 * [概述](#5.1)
 * [配置](#5.2)

* [声学所接口](#6)  
 
 * [概述](#6.1)
 * [配置](#6.2)

* [为mongo创建索引](#7)


--- 
 
<h2 id="4">基础环境搭建</h2>

本项目的开发环境为 Centos，故以Centos 环境搭建为例，开发环境的centos 版本为 6.7  。假定当前系统为未做任何改变的原生操作系统。

```
LSB Version:	:base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID:	CentOS
Description:	CentOS release 6.7 (Final)
Release:	6.7
Codename:	Final
```
  
<h3 id="4.2"> 更换源 </h3>  

Ubuntu 环境下建议更换apt 源为阿里源，Centos 环境下建议更换 yum 源为阿里源。 
Centos 的更换方法为  

```
mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
```

下载相应的centos 源并放在目录 /etc/yum.repos.d/下。
执行

```  
yum clean all
yum makecache

```  

<h3 id="4.1">python 环境搭建</h3>  

<h4 id="4.1.1">python 版本升级</h4>  

Centos 的python 版本为 2.6，（ubuntu 和 macos版本为2.7 故可以省略此步）。  
开发过程中此步骤的做法是是用二进制包安装  

* 卸载原有的python程序
* 从安装包安装，网上有很多相关的教程，不详细描述

<h4 id="4.1.2">python环境与python依赖</h4>  
开发过程中所有的程序均使用同一个python 的虚拟环境，建议在实际运行中进程使用虚拟环境运行。所有进程均不在root 权限下运行，例如在测试机上新建了一个 crawler 账户，在crawler 账户下执行所有程序。虚拟环境安装步骤如下：

* 安装 virtualenv
* 运行 virtualenv envdir
* 运行 source envdir/bin/activate
* 运行pip install requirements.txt 解决依赖问题

所有python 进程均可运行在当前的虚拟环境中。  

每个源代码包中均有 requirements.txt 文件。如果丢失，项目的requirements 文件如下：

```  
appnope==0.1.0
attrs==15.2.0
backports-abc==0.4
backports.ssl-match-hostname==3.5.0.1
certifi==2016.2.28
cffi==1.5.2
cryptography==1.2.3
cssselect==0.9.1
decorator==4.0.9
Django==1.9.3
enum34==1.1.2
gnureadline==6.3.3
greenlet==0.4.9
guppy==0.1.10
hiredis==0.2.0
idna==2.0
ipaddress==1.0.16
ipython==4.1.2
ipython-genutils==0.1.0
Jinja2==2.8
jsbeautifier==1.6.2
lxml==3.5.0
MarkupSafe==0.23
motor==0.6.2
numpy==1.10.4
path.py==8.1.2
pdb==0.1
pexpect==4.0.1
pickleshare==0.6
Pillow==3.1.1
ptyprocess==0.5.1
pyasn1==0.1.9
pyasn1-modules==0.0.8
pycparser==2.14
pymongo==2.8
pyOpenSSL==0.15.1
python-gnupg==0.3.8
pytz==2016.3
PyYAML==3.11
queuelib==1.4.2
redis==2.10.5
requests==2.9.1
requests-mock==0.7.0
Scrapy==1.0.5
scrapy-djangoitem==1.0.0
scrapyjs==0.1.1
service-identity==16.0.0
simplegeneric==0.8.1
singledispatch==3.4.0.3
six==1.10.0
tornado==4.3
traitlets==4.1.0
Twisted==15.5.0
w3lib==1.13.0
xmltodict==0.10.1
zope.interface==4.1.3

```

<h3 id="4.2">nodejs 环境搭建</h3>

Centos 下不包含nodejs 的运行环境，开发环境使用如下链接方式搭建环境 ：  
[Centos下nodejs环境安装](http://www.jianshu.com/p/783906f10d58)  
开发环境nodejs 版本为：

```  
[crawler@localhost sxs_spider]$ node -v
v4.4.4
```

<h3 id="4.3">mongo 安装</h3>
[Centos下mongo的安装](https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/) 

<h3 id="4.4">supervisor 安装</h3>  
开发环境下supervisor 使用pip 安装，需要退出虚拟环境，并使用root 权限安装。  

```  
sudo pip install supervisor 
```

<h3 id="4.5"> crontab 配置</h3> 

<h3 id="4.6"> 防火墙配置 </h3> 


<h2 id="1">点播爬虫</h2>  

<h3 id="1.1">点播爬虫概述</h3>  

点播爬虫由三部分组成，分别完成如下功能:

* 爬取三个网站的元数据，并保存到mongo 中，并在爬取的同时实时更新爬取量  

<h3 id="1.2">点播爬虫部署</h3>  

* 在开发中，代码部署使用git，但是考虑到目前项目组没有公用的git 仓库，故可以使用 scp 向服务器上传代码。项目的svn 地址为  [这是一个svn地址]() 

<h3 id="1.3">点播爬虫配置</h3> 

* 上传代码之后需要更改配置文件，配置文件的地址为 ./m_spider/settings.py ,配置文件内容为：

```  
# -*- coding: utf-8 -*-

# Scrapy settings for m_spider project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     http://doc.scrapy.org/en/latest/topics/settings.html
#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html
#日志配置
#为 root 添加一个Handler,logger

import logging
debug_log = logging.FileHandler(
        filename='./log/debug.log'
)
debug_log.setLevel(logging.WARNING)
logging.getLogger('').addHandler(debug_log)

SPIDER_MODULES = ['m_spider.spiders']
NEWSPIDER_MODULE = 'm_spider.spiders'

#设置splash 代理
#SPLASH_URL = 'http://192.168.99.100:8050'

#DOWNLOADER_MIDDLEWARES = {
#    'scrapyjs.SplashMiddleware': 725,
#}

#DUPEFILTER_CLASS = 'scrapyjs.SplashAwareDupeFilter'

#spider 与Django project结合，使用了Django 中的models
#做了如下更改

#import sys
#sys.path.append('/Users/xiyuanbupt/PycharmProjects/x_music/music_admin')
##print sys.path
#import os
#os.environ['DJANGO_SETTINGS_MODULE'] = 'music_admin.settings'
#import django
#django.setup()

#redis 设置
#内容分别为主机，端口，哪个数据库，密码
#REDIS_HOST = 'localhost'
#REDIS_PORT = 6379
#REDIS_DB = 0

#REDIS_PASS = None

#MongoDB 设置
MONGO_URI = 'mongodb://localhost:27017'
MONGO_DATABASE = 'test_spider'

#设置下载图片存储的位置
IMAGES_STORE = '/var/crawler/images'
FILES_STORE = '/var/crawler/audios'

XMLY_SETTINGS= {
        "IMAGES_STORE":'/var/crawler/xmly/images',
        "FILES_STORE":'/var/crawler/xmly/audios'
}
KL_SETTINGS = {
        "IMAGES_STORE":'/var/crawler/kl/images',
        "FILES_STORE":'/var/crawler/kl/audios'
}
QT_SETTINGS = {
        "IMAGES_STORE":'/var/crawler/qt/images',
        "FILES_STORE":'/var/crawler/qt/audios'
}
QINGTINGCONF = {
        'allowdomains' : ['qingting.fm'],
        'start_urls' : ['http://www.qingting.fm'],
        'playPR' : 'http://od.qingting.fm',
        'logfile' : 'qingting.log',
        'collection' : 'qingting_item',
        'qt_c_settings':{
        "IMAGES_STORE":'/var/crawler/qt/images',
        "FILES_STORE":'/var/crawler/qt/audios',
        }
}

# Crawl responsibly by identifying yourself (and your website) on the user-agent

#设置自定义限速

#USER_AGENT = 'm_spider (+http://www.yourdomain.com)'

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS=32

# Configure a delay for requests for the same website (default: 0)
# See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY=3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN=16
#CONCURRENT_REQUESTS_PER_IP=16

# Disable cookies (enabled by default)
COOKIES_ENABLED=False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED=False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
#}

# Enable or disable spider middlewares
# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    'm_spider.middlewares.MyCustomSpiderMiddleware': 543,
#}
#SPIDER_MIDDLEWARES = {
#        'm_spider.middlewares.SaveInfoToDB':200,
#}

# Enable or disable downloader middlewares
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html
#DOWNLOADER_MIDDLEWARES = {
#    'm_spider.middlewares.MyCustomDownloaderMiddleware': 543,
#}

# Enable or disable extensioXmlyAudiosDownloader':747,
# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    'scrapy.telnet.TelnetConsole': None,
#}

# Configure item pipelines
# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
#注意 itempipline 的优先级是 先过数字最小的，之后过数字对打的
ITEM_PIPELINES = {
#存储到 mongodb 中
        'm_spider.pipelines.SaveToMongo':800,
#        'm_spider.pipelines.KlimageDownloader':750,
#        'm_spider.pipelines.KlAudioDownloader':749,
#        'm_spider.pipelines.XmlyImageDownloader':748,
#        'm_spider.pipelines.XmlyAudiosDownloader':747,
}

# Enable and configure the AutoThrottle extension (disabled by default)
# See http://doc.scrapy.org/en/latest/topics/autothrottle.html
# NOTE: AutoThrottle will honour the standard settings for concurrency and delay
#设置限速
AUTOTHROTTLE_ENABLED=True
# The initial download delay
AUTOTHROTTLE_START_DELAY=5
# The maximum download delay to be set in case of high latencies
AUTOTHROTTLE_MAX_DELAY=60
# Enable showing throttling stats for every response received:
AUTOTHROTTLE_DEBUG=False

# Enable and configure HTTP caching (disabled by default)
# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED=True
#HTTPCACHE_EXPIRATION_SECS=0
#HTTPCACHE_DIR='httpcache'
#HTTPCACHE_IGNORE_HTTP_CODES=[]
#HTTPCACHE_STORAGE='scrapy.extensions.httpcache.FilesystemCacheStorage'
#如下代码为设置随机用户代理
#USER_AGENT_LIST="/home/crawler/m_spider/m_spider/etc/user_agents.txt"

```

其中 14 - 19 行内容为配置debug 日志使用，程序所有的错误日志都会打到对应的文件中，在调试过程中可以通过查看日志了解爬虫运行情况，并解决相关的问题。

* MONGO_URI 遵循mongo 的uri 格式，地址，端口，和认证信息都可以在其中添加
* MONGO_DATABASE 为爬虫使用的db ，爬虫爬取到的元信息会增加到此数据库中
* IMAGES_SOTRE 为通用的设置，在三个爬虫中均未使用
* FIELS_STORE 为通用设置，可以不用更改
* XMLY_SETTINGS 为 xmly 爬虫设置了文件保存的位置，建议先建立目录，并设置。音频文件会保存到 FILES_STORE 中
* KL_SETTINGS 为kl 的设置项，与上面相同
* QT_SETTINGS 为qt 的设置项，与上面相同
* 因开发人员不同，所以新增了 QINGTINGCONF 设置项，具体含义与上面类似
* 在设置中为了防止爬虫被封，故建议禁止cookie ，禁止cookie 的选项为  cookies_enabled=false
* 其他配置项基本不用更改。

<h3 id="1.4">调试运行</h3>

爬虫依赖supervisor 与 crontab 运行，但是在提交给supervisor 运行前需要检查爬虫是否有bug，首先需要在console 中手动运行检查，运行方式为

* 检查都有哪些爬虫  

```  
(m_spider) xiyuanBuptdeMacBook-Pro-3574:m_spider xiyuanbupt$ scrapy list
kaolafm
nnn
qingting
splash_kaola
ximalaya.com
xmly
```

* 运行当前目录下的爬虫并肉眼检查是否有bug

```  
(m_spider) xiyuanBuptdeMacBook-Pro-3574:m_spider xiyuanbupt$ scrapy crawl kaolafm  
```

* 其他爬虫类似，如果有bug 需要找相应的开发人员解决bug

<h3 id="1.5">配置supervisor 与crontab</h3> 

爬虫启停，运行，关闭均通过supervisor 管理，通过pip 安装的supervisor 的配置文件位置为 /etc/supervisor supervisor 的配置需要根据实际情况来决定，（例如，爬虫是否需要记录爬取情况可以被命名为不同的进程，如：喜马拉雅的配置情况如下）

```  
 [program:xmly]
 directory=/home/crawler/sxs_spider
 autostart=false
 command=scrapy crawl xmly -o /var/crawler/raw_data/xmly.json
 startsecs=1
 startretries=3
 autorestart=false
 user=crawler
 redirect_stderr=true
 stdout_logfile=/var/crawler/logs/xmly/crawl.log
 stdout_logfile_maxbytes=30MB
 stdout_logfile_backups=10
 stderr_logfile=/var/crawler/logs/xmly/crawl_err.log
 stderr_logfile_maxbytes=30MB
 stdout_logfile_backups=10
``` 

上面的配置项大概的含义是（建议查看supervisor官方文档）：
给爬虫进程命名为xmly ，并可以使用相关命令执行开始暂停等工作，将爬虫获得item 持久换存储到 /var/crawler/raw_data/xmly.json 中，爬虫的日志保存在 /var/crawler/logs/xmly/crawl.log ，爬虫的错误日志被保存在 /var/crawler/logs/xmly/crawl_err.log 中。

启动喜马拉雅爬虫的命令为  (需要使用root 用户)

```  
supervisorctl start xmly
```

爬虫的暂停与开始爬取可以通过telnet来控制，具体控制方式建议查看scrapy官方文档  
如果想要使得爬虫有记忆的爬取需要使用如下的配置项   scrapy crawl somespider -s JOBDIR=crawls/somespider-1，这样爬虫在使用 ctl-c 停止，再次开始的时候爬虫可以从上次暂停的地方继续爬取。

*crontab*  
爬虫的定期执行使用contab ，在开发过程中，每周三和每周六周期性的执行爬虫，crontab 的配置为  

```  
   2 0 23 * * 2,5 supervisorctl restart all
```

具体配置视具体环境来决定。


<h2 id="2">点播媒体文件下载</h2>

媒体文件下载单独使用一个进程，媒体文件下载包含四个爬虫，四个爬虫分别为：

* 喜马拉雅音频文件的爬虫
* 蜻蜓音频文件的爬虫
* 考拉音频文件的爬虫
* 直播网址图片的爬虫

爬虫的大致思想是遍历元数据爬虫的数据库，获得相关的网址，并通过爬虫中间件下载相关文件，并更新元数据库，将文件的下载地址插入到数据库中。

爬虫的部署和测试与[点播爬虫](#1)相同，在本项目的四个爬虫如下：

```  
(m_spider) xiyuanBuptdeMacBook-Pro-3574:file_downloader xiyuanbupt$ scrapy list
kl_audio
live_image
qt_audio
xmly_audio  
```

爬虫启停与调试，supervisor 与crontab 控制与[点播爬虫](#1) 相同 

<h2 id="3">统计页面后台</h2>

<h3 id="3.1">概述</h3>

统计页面后台程序使用python 编写，具体思想为遍历数据库，统计需要的指标，并将相关数据插入到数据库中，程序入口为main，并通过crontab 来控制，开发环境中crontab 的配置为。


```   

1 0 * * * * /usr/local/bin/python /home/crawler/statistics_task/main.py  

```  

<h3 id="3.2"> 配置</h3> 

配置文件被保存在 ./global.ini 中，配置内容为： 

```  
;The conf of cnr crawler statistics

[kaola]
;音频文件保存的位置与媒体文件保存的位置，用来统计当前媒体量
audio_dir = /var/crawler/kl/audios/full
image_dir = /var/crawler/kl/images/full

[qingting]
audio_dir = /var/crawler/qt/audios/full
image_dir = /var/crawler/qt/images/full

[ximalaya]
audio_dir = /var/crawler/xmly/audios/full
image_dir = /var/crawler/xmly/images/full

[mongo]
;mongo 的配置，statistics_db 用来保存统计后的结果信息，spider_db 为爬虫爬取元数据的数据库
ip = localhost
port = 27017
statistics_db = crawl_info
spider_db = test_spider


[spider_collections]
;爬虫元数据的collections
crontab = crontab_result
xmly_album = xmly_album
xmly_category = xmly_category
xmly_audio = xmly_audio
kl_album = kl_album
kl_audio = kl_audio
kl_category = kl_category
qt_item = qt_item
qt_audio = qt_audio


[statistics_collections]
；保存统计结果的 collections
media_summary = media_summary
crawl_history = crawl_history

[test] 
;用来测试的路径
logDir = /var/crawler/logs/live/kaolaEPG.log
crawler = kaolaEPG
```

详细说明已经在配置相关位置中说明

<h2 id="5">统计页面前端接口</h2>  

<h3 id="5.1">统计页面前端接口概述</h3> 

统计页面后台使用nodejs 完成，之所以未使用php 是因为不想引入apache，未使用python 是因为python 使用mongo 的查询需要加很多的引号。
统计页面使用npm 来管理包依赖于进程的运行情况  


**安装** npm install   
**启动** npm start   

在npm 中使用的是开发模式。  
在实际使用中需要通过supervisor 来管理进程。

<h3 id="5.2">统计页面前端接口配置</h3>   
统计页面因为配置项较少（只有一个数据库连接），故为单独提出配置项，数据库配置在 app.js 代码中。

<h2 id="6">推送媒体文件接口与接收反馈接口</h2>

<h3 id="6.1">接口概述</h3> 

本接口为向cnr 推送爬取结果的接口，并接受cnr 反馈的接口，全部使用python 开发。同时考虑了高并发的情况，在反馈接口中使用了tornado 与 motor，故需要重新安装相应的依赖。
程序的入口为sender ，反馈接口是一个 httpserver,在命令行下直接使用   

``` 
python receiver.py 启动运行
```

receiver 需要使用supervisor 进行管理启动。

<h3 id="6.2">配置</h3>

所有的配置文件均在ini 格式的文档中  

<h2 id="7">为mongo创建索引</h2> 

此步骤可以忽略，但是不为mongo 相关的字段创建索会给mongo 造成大量的压力，致使mongo 使用大量的cpu 和内存资源。mongo索引的创建建议在部署环境中写脚本来创建索引。  
